---
title: "ContextBERT: Contextual Graph Representation Learning in Text Disambiguation"
collection: publications
permalink: /publication/2019-10-01-paper-title-number-1
excerpt: 'This paper is about solving word sense disambiguation task using graph convolutional networks.'
date: 2021-10-01
venue: 'European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases.(ECML PKDD)'
paperurl: 'https://ceur-ws.org/Vol-2997/paper2.pdf'

citation: 'Saeidi, Mozhgan. (2021). &quot; ContextBERT: Contextual Graph Representation Learning in Text Disambiguation.&quot; Booktitle: PKDD, Germany, September 16--20, 2019, Proceedings, Part II. pages={283--297}, year={2021}, organization={Springer} <i>Journal 1</i>. 1(1).'
---
Word representations derived by neural language models have been
shown to effectively carry useful semantic information to improve the final results
of various Natural Language Processing tasks. The information provided by these
representations encodes the subtle distinction that might occur between different
meanings of the same word. However, these representations do not include the
input textâ€™s information, as the context, and a semantic knowledge base network.
This integration of context and semantic network is helpful in NLP tasks, specifically in the lexical ambiguity problem. In this paper, we first analyze the defects
of current state-of-the-art representation learning approaches, and second, we
present a word representation learning method, named ContextBERT, that is aware
of the semantic knowledge base network and the context. ContextBERT is a novel
approach to producing sense embeddings for the lexical meanings within a lexical
knowledge base, using a pre-trained BERT model The novel difference in our representation is the integration of the knowledge base information and the input text.
Our representations enable a simple 1-Nearest-Neighbour algorithm to perform
state-of-the-art models in the English Word Sense Disambiguation task.
