---
title: "Benchmarks for Physical Reasoning AI"
collection: publications
permalink: /publication/2019-10-01-paper-title-number-1
excerpt: 'This paper is about Physical Reasoning in AI. Physical reasoning, a crucial aspect of human intelligence, allows us to understand and predict how objects move and interact in the world. AI research is actively developing models that can reason about physics, enabling robots to grasp objects, navigate environments, and perform tasks in the real world. Mastering physical reasoning is a key challenge in Artificial Intelligence. This ability to understand the physical laws governing objects is essential for robots to operate safely and effectively in our complex world.'
date: 2023-10-01
venue: 'Transactions on Machine Learning Research(TMLR)'
paperurl: 'https://arxiv.org/pdf/2312.10728.pdf'

citation: 'Andrew Melnik, Robin Schiewer, Moritz Lange, Andrei Muresanu, Mozhgan Saeidi, Animesh Garg, Helge Ritter. (2023). &quot; journal={TMLR}, pages={1--50}, year={2023}, publisher={TMLR} <i>Journal 1</i>. 1(1).'
---
Physical reasoning is a crucial aspect in the development of general AI systems, given that human learning starts with interacting with the physical world before progressing to more complex concepts. Although researchers have studied and assessed the physical reasoning of AI approaches through various specific benchmarks, there is no comprehensive approach to evaluating and measuring progress. Therefore, we aim to offer an overview of existing benchmarks and their solution approaches and propose a unified perspective for measuring the physical reasoning capacity of AI systems. We select benchmarks that are designed to test algorithmic performance in physical reasoning tasks. While each of the selected benchmarks poses a unique challenge, their ensemble provides a comprehensive proving ground for an AI generalist agent with a measurable skill level for various physical reasoning concepts. This gives an advantage to such an ensemble of benchmarks over other holistic benchmarks that aim to simulate the real world by intertwining its complexity and many concepts. We group the presented set of physical reasoning benchmarks into subcategories so that more narrow generalist AI agents can be tested first on these groups.
